---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# gdp

<!-- badges: start -->
[![R-CMD-check](https://github.com/KTH-Library/gdp/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/KTH-Library/gdp/actions/workflows/R-CMD-check.yaml)
[![Lifecycle: experimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://lifecycle.r-lib.org/articles/stages.html#experimental)
<!-- badges: end -->

The goal of gdp is to provide some tools and resources which are useful when working with data from the GDP project.

The client provided in the `gdp` R-package accesses data from an API described here:

- https://portal.api.vinnova.se/gdp-openapi-dokumentation
- https://gdpswagger.vinnova.se/
- https://portal.api.vinnova.se/gdp-openapi-dokumentation

Access to the API can be requested at https://portal.api.vinnova.se. This client makes use of a system account which provides access to the open data provided through the API.

Version 1 of the API conforms to this information model:

## Information model

![GDP information schema v 1](man/figures/gdp_schema_v1.png)

## Installation

You can install the development version of gdp from [GitHub](https://github.com/) with:

``` r
# install.packages("devtools")
devtools::install_github("KTH-Library/gdp")
```

## Examples

This is an example showing how to convert data from the GDP API to rectangular data which can be persisted locally in a relational database:

```{r example, eval=FALSE}
library(gdp)

my_orgid <- "202100-3054"  # KTHs organisation id used in GDP

# harvest data for the given organisation identifier
harvest <- gdp_harvest(organisation = my_orgid)

# export the data into a local database (duckdb, see https://duckdb.org)
destination_dir <- gdp_export_database(harvest)

# upload the database to S3 object storage (given a specific S3 alias / path specification)
gdp_upload_files(sourcedir = destination_dir, s3_targetdir = "kthb/projects/gdp")

# export the resulting tables individually as .csv and .parquet
gdp_export_tables(harvest, destdir = "/tmp/gdp")

# upload the csv and parquet files to S3
gdp_upload_files(sourcedir = "/tmp/gdp", s3_targetdir = "kthb/projects/gdp")

```

This example shows how to search for proposals/applications across a few different topic areas:

```{r, eval=FALSE}
library(gdp)
library(tidyverse)

# use filters to extract data

my_orgid <- "202100-3054"  # KTHs organisation id used in GDP

my_proposals <- 
  gdp_proposals(filter = gdp_filter(type = "proposals", org_id = my_orgid)) |> 
  gdp:::to_tbls_proposals()

# proposals related to transport systems and logistics or has code which begins w "2.1.04"
my_proposals$topics |> 
  filter(topic_name_eng == "Transport Systems and Logistics" | 
    grepl("^2[.]1[.]0[4]", topic_code)) |> 
  distinct(id_proposal, topic_code, topic_name_eng) |> 
  left_join(my_proposals$proposals) |> 
  select(id_proposal, title, req_amount, topic_code) |> 
  arrange(desc(req_amount))

```

## Harvesting from GDP data providers that implement the "lite" JSON response format

If a provider uses the "lite" JSON response format, a couple of functions allow for retrieving all data in one request in tabular form:

```{r, eval=FALSE}
library(gdp)
library(tidyverse)

read_gdp_lite(
  url = "https://data.bibliometrics.lib.kth.se/projects/em/fundings.json", 
  entity = "finansieradeaktiviteter"
)

read_gdp_lite(
  url = "https://data.bibliometrics.lib.kth.se/projects/em/calls.json", 
  entity = "utlysningar"
)

read_gdp_lite(
  url = "https://data.bibliometrics.lib.kth.se/projects/em/proposals.json", 
  entity = "ansokningar"
)

```

For such a provider, all data can be harvested and exported in one go as a single duckdb database file:

```{r, eval=FALSE}
library(gdp)
library(tidyverse)

exported_db <- 
  write_gdp_lite(
    url_proposals = "https://data.bibliometrics.lib.kth.se/projects/em/proposals.json",
    url_calls = "https://data.bibliometrics.lib.kth.se/projects/em/calls.json",
    url_fundings = "https://data.bibliometrics.lib.kth.se/projects/em/fundings.json"
  )

# by default the exported_db will be located in the temp directory

```

After doing such a harvest, results can be transferred to S3 storage for remote access or the resulting db can be queried (locally or remotely) with tools like:

- https://sql-workbench.com 
- https://shell.duckdb.com
- https://harlequin.sh/

This single file database format can be used from R, Python, NodeJS and many other programming languages.

